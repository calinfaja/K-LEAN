# K-LEAN LiteLLM Configuration - Ollama (Local)
# ==============================================
# No API key needed - local models only
# Usage: ollama pull <model> && litellm --config ~/.config/litellm/config.yaml

litellm_settings:
  drop_params: true

# Model aliases
model_alias_map:
  "claude-sonnet-4-5-20250929": "ollama-coder"
  "claude-3-5-sonnet-20241022": "ollama-coder"
  "claude-3-haiku-20240307": "ollama-coder"
  "claude-3-opus-20240229": "ollama-coder"
  "sonnet": "ollama-coder"
  "haiku": "ollama-coder"
  "opus": "ollama-coder"

model_list:
  # Ollama local models (no API key needed)
  # Requirements: ollama pull <model-name>

  - model_name: ollama-coder
    litellm_params:
      model: ollama/mistral-nemo
      api_base: http://localhost:11434
      drop_params: true

  - model_name: ollama-reasoning
    litellm_params:
      model: ollama/deepseek-r1:14b
      api_base: http://localhost:11434
      drop_params: true

  - model_name: ollama-tools
    litellm_params:
      model: ollama/neural-chat
      api_base: http://localhost:11434
      drop_params: true

  - model_name: ollama-research
    litellm_params:
      model: ollama/mistral:latest
      api_base: http://localhost:11434
      drop_params: true

  - model_name: ollama-agent
    litellm_params:
      model: ollama/llama2-uncensored
      api_base: http://localhost:11434
      drop_params: true

  - model_name: ollama-uncensored
    litellm_params:
      model: ollama/llama2-uncensored
      api_base: http://localhost:11434
      drop_params: true

# Setup Instructions:
# 1. Install ollama: https://ollama.ai
# 2. Start ollama: ollama serve
# 3. Pull models: ollama pull mistral-nemo deepseek-r1:14b neural-chat
# 4. Start litellm: litellm --config ~/.config/litellm/config.yaml
